{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import artm\n",
    "import pymystem3\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "mystem = pymystem3.Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\sbt-shekhovtsov-\n",
      "[nltk_data]     rv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\sbt-shekhovtsov-\n",
      "[nltk_data]     rv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# work environment setup - one-time actions\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мама мыла раму'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # remove numbers, single-char and stop-words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [w for w in words if w.isalpha() and len(w)>1] # and w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "clean_text('мама мыла раму')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAKE STOP-WORDS DATASET\n",
    "def make_stop_words_dataset():\n",
    "    stop_words= nltk.corpus.stopwords.words('russian')\n",
    "    names = pd.read_csv('names.csv')\n",
    "    stop_words.extend(names['name'])\n",
    "    #stop_words = list(map(lambda x: stemmer.stem(x), stop_words))\n",
    "    #df = pd.read_csv('stop-words-russian-nltk.csv')\n",
    "    df = pd.DataFrame(stop_words, columns=['stop-word'])\n",
    "    #df['stop-word'] = df['stop-word'].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    df.to_csv('stop-words-russian.csv', index=False, header=['stop-word'])\n",
    "\n",
    "make_stop_words_dataset()\n",
    "\n",
    "#stemmer = SnowballStemmer('russian')\n",
    "# stop_word=\" \"\n",
    "# for i in stop_words:\n",
    "#         stop_word=  stop_word+\" \"+i\n",
    "# print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop-word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>и</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>в</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>во</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>не</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>он</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>на</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>я</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>с</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>со</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>как</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>а</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>то</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>все</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>она</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>так</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>его</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>но</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>да</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ты</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>к</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>у</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>же</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>вы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>за</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>бы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>по</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>только</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ее</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>мне</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900</th>\n",
       "      <td>юрик</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4901</th>\n",
       "      <td>юрис</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4902</th>\n",
       "      <td>юрист</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4903</th>\n",
       "      <td>юрій</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904</th>\n",
       "      <td>юрік</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4905</th>\n",
       "      <td>юрка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4906</th>\n",
       "      <td>юрко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4907</th>\n",
       "      <td>юрок</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4908</th>\n",
       "      <td>юрчик</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4909</th>\n",
       "      <td>юсиф</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4910</th>\n",
       "      <td>юсуп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4911</th>\n",
       "      <td>юсуф</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4912</th>\n",
       "      <td>яков</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4913</th>\n",
       "      <td>якуб</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914</th>\n",
       "      <td>ямиль</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4915</th>\n",
       "      <td>ян</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>яник</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917</th>\n",
       "      <td>янис</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>януш</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>янчик</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>яр</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>ярик</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>ярік</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>яромир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>ярослав</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>ясин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>яха</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>яша</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4928</th>\n",
       "      <td>яшар</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>яшка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4930 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     stop-word\n",
       "0            и\n",
       "1            в\n",
       "2           во\n",
       "3           не\n",
       "4          что\n",
       "5           он\n",
       "6           на\n",
       "7            я\n",
       "8            с\n",
       "9           со\n",
       "10         как\n",
       "11           а\n",
       "12          то\n",
       "13         все\n",
       "14         она\n",
       "15         так\n",
       "16         его\n",
       "17          но\n",
       "18          да\n",
       "19          ты\n",
       "20           к\n",
       "21           у\n",
       "22          же\n",
       "23          вы\n",
       "24          за\n",
       "25          бы\n",
       "26          по\n",
       "27      только\n",
       "28          ее\n",
       "29         мне\n",
       "...        ...\n",
       "4900      юрик\n",
       "4901      юрис\n",
       "4902     юрист\n",
       "4903      юрій\n",
       "4904      юрік\n",
       "4905      юрка\n",
       "4906      юрко\n",
       "4907      юрок\n",
       "4908     юрчик\n",
       "4909      юсиф\n",
       "4910      юсуп\n",
       "4911      юсуф\n",
       "4912      яков\n",
       "4913      якуб\n",
       "4914     ямиль\n",
       "4915        ян\n",
       "4916      яник\n",
       "4917      янис\n",
       "4918      януш\n",
       "4919     янчик\n",
       "4920        яр\n",
       "4921      ярик\n",
       "4922      ярік\n",
       "4923    яромир\n",
       "4924   ярослав\n",
       "4925      ясин\n",
       "4926       яха\n",
       "4927       яша\n",
       "4928      яшар\n",
       "4929      яшка\n",
       "\n",
       "[4930 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     stop_words= nltk.corpus.stopwords.words('russian')  \n",
    "#     names = pd.read_csv('names.csv')\n",
    "#     stop_words.extend(names['name'])\n",
    "#df = pd.DataFrame(stop_words, columns=['stop-word'])\n",
    "#df['stop-word'] = df['stop-word'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "# load raw data\n",
    "def load_data():\n",
    "    NROWS = None\n",
    "    df = pd.read_csv( 'vk.csv', nrows=NROWS).fillna('')\n",
    "    return df\n",
    "\n",
    "\n",
    "# # stemming\n",
    "# def stemming(df):\n",
    "#     stemmer = SnowballStemmer('russian')\n",
    "#     df['question_stem'] = df['question'].apply(lambda x: ' '.join([stemmer.stem(w) for w in str(x).split()]))\n",
    "#     df['answer_stem'] = df['answer'].apply(lambda x: ' '.join([stemmer.stem(w) for w in str(x).split()]))\n",
    "#     # df[['question_stem','answer_stem']]\n",
    "#     # df.to_csv('vk-stemmed.csv', index=False, header=True)\n",
    "\n",
    "\n",
    "def lemmatize(strings):\n",
    "    s = mystem.lemmatize('||'.join(strings))\n",
    "    s = ''.join(s)\n",
    "    return s.split('||')\n",
    "\n",
    "\n",
    "def data_preparation():\n",
    "    df = load_data()\n",
    "    # stemming(df)    \n",
    "    \n",
    "    df['question_clean'] = lemmatize(df['question'].values)\n",
    "    df['answer_clean'] = lemmatize(df['answer'].values)\n",
    "\n",
    "    df['question_clean'] = df['question_clean'].apply(lambda x: clean_text(x))\n",
    "    df['answer_clean'] = df['answer_clean'].apply(lambda x: clean_text(x))\n",
    "    df.to_csv('vk-cleaned.csv', index=False, columns=['question_clean','answer_clean'], header=['question','answer'])\n",
    "\n",
    "# remove punctuation & stop-words\n",
    "stop_words = list(pd.read_csv('stop-words-russian.csv')['stop-word'])\n",
    "data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load_data()\n",
    "# l = lemmatize(df['question'].values[:100])\n",
    "# l2 = list(map(clean_text, l))\n",
    "# l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to C:\\Users\\sbt-shekhovtsov-rv/.local/bin\\mystem.exe from http://download.cdn.yandex.net/mystem/mystem-3.1-win-64bit.zip\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e2f482e2d592>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmystem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpymystem3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sber_topics_raw.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_topics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_topics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#df_topics['topic'] = df_topics['topic'].apply(lambda x: clean_text(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#df_topics.head(200)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\p\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3192\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e2f482e2d592>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmystem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpymystem3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sber_topics_raw.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_topics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_topics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#df_topics['topic'] = df_topics['topic'].apply(lambda x: clean_text(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#df_topics.head(200)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "# SBER TOPICS\n",
    "# stemmer = SnowballStemmer('russian')\n",
    "mystem = pymystem3.Mystem()\n",
    "df_topics = pd.read_csv('sber_topics_raw.csv', names=['topic'], sep='\\n')\n",
    "df_topics['topic'] = df_topics['topic'].apply(lambda x: clean_text(x))\n",
    "#df_topics['topic'] = df_topics['topic'].apply(lambda x: clean_text(x))\n",
    "#df_topics.head(200)\n",
    "df_topics.to_csv('sber_topics.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>кредитн карт моментум расплачива зарубежн инте...</td>\n",
       "      <td>можете использова кредит карт cred momentum оп...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>дебетн карт пришл сообщен неразрешен задолженн...</td>\n",
       "      <td>вопрос обрат служб помощ банк телефон специали...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>вопрос поч приход смс пополнен карты рад подкл...</td>\n",
       "      <td>вопрос обрат служб помощ банк телефон специали...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>зарплатн карт сб польз сбол кром перевод частн...</td>\n",
       "      <td>вопрос обрат служб помощ банк телефон специали...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>пришл смс одобр предворительн кредит документ ...</td>\n",
       "      <td>потребительск кред выда соблюден след условий ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  кредитн карт моментум расплачива зарубежн инте...   \n",
       "1  дебетн карт пришл сообщен неразрешен задолженн...   \n",
       "2  вопрос поч приход смс пополнен карты рад подкл...   \n",
       "3  зарплатн карт сб польз сбол кром перевод частн...   \n",
       "4  пришл смс одобр предворительн кредит документ ...   \n",
       "\n",
       "                                              answer  \n",
       "0  можете использова кредит карт cred momentum оп...  \n",
       "1  вопрос обрат служб помощ банк телефон специали...  \n",
       "2  вопрос обрат служб помощ банк телефон специали...  \n",
       "3  вопрос обрат служб помощ банк телефон специали...  \n",
       "4  потребительск кред выда соблюден след условий ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('vk-cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf = TfidfVectorizer()\n",
    "# x = tfidf.fit_transform(df['question'].fillna(''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = list(pd.read_csv('stop-words-russian-nltk.csv')['stop-word'])\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# countVectorizer = CountVectorizer(max_df=0.5, min_df=int(2),\n",
    "#                                 max_features=10000,\n",
    "#                                 stop_words=stop_words)\n",
    "# x = countVectorizer.fit_transform(df['question'].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40117, 10000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=250, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# lda = LatentDirichletAllocation(n_components=250, max_iter=5,\n",
    "#                                 learning_method='online',\n",
    "#                                 learning_offset=50.,\n",
    "#                                 random_state=0)\n",
    "# lda.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def print_top_words(model, feature_names, n_top_words):\n",
    "#     for topic_idx, topic in enumerate(model.components_):\n",
    "#         message = \"Topic #%d: \" % topic_idx\n",
    "#         message += \" \".join([feature_names[i]\n",
    "#                              for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "#         print(message)\n",
    "#     print()\n",
    "    \n",
    "# feature_names = countVectorizer.get_feature_names()\n",
    "# print_top_words(lda, feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "# x = NMF(n_components=250, random_state=1, alpha=.1, l1_ratio=.5).fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vw(df, columns_list, csv_name):\n",
    "    df['vw_index'] = df.index.values\n",
    "    df['vw_index'] = df['vw_index'].apply(lambda x: str(x))\n",
    "    df['vw_text'] = 'd' + df['vw_index']\n",
    "    for column_name in columns_list:\n",
    "        df['vw_text'] += ' ' + df[column_name]\n",
    "    df[['vw_text']].to_csv(csv_name, header=False, index=False)\n",
    "    \n",
    "df_topics = pd.read_csv('sber_topics.csv')\n",
    "to_vw( df, ['question', 'answer'], 'vk.vw')\n",
    "to_vw(df_topics, ['topic'], 'sber_topics.vw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ARTM batches & dictionary - one-time action\n",
    "batch_vectorizer = artm.BatchVectorizer(\n",
    "    data_path = 'vk.vw',\n",
    "    data_format='vowpal_wabbit',\n",
    "    target_folder='artm')\n",
    "\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.gather(data_path='artm')\n",
    "dictionary.save(dictionary_path='artm/dictionary')\n",
    "dictionary.save_text(dictionary_path='artm/dictionary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='artm',\n",
    "                                        data_format='batches')\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.load(dictionary_path='artm/dictionary.dict')\n",
    "model = artm.ARTM(num_topics=200, dictionary=dictionary, cache_theta=True)\n",
    "model.scores.add(artm.PerplexityScore(name='perplexity_score', dictionary=dictionary))\n",
    "model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))\n",
    "model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))\n",
    "model.scores.add(artm.TopTokensScore(name='top_tokens_score'))\n",
    "model.num_document_passes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=25)\n",
    "#model.score_tracker['perplexity_score'].last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.score_tracker['perplexity_score'].value)      # .last_value\n",
    "print()\n",
    "print(model.score_tracker['sparsity_phi_score'].value)    # .last_value\n",
    "print()\n",
    "print(model.score_tracker['sparsity_theta_score'].value)  # .last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for topic_name in model.topic_names:\n",
    "    print(\n",
    "        topic_name + ':',\n",
    "        model.score_tracker['top_tokens_score'].last_tokens[topic_name]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_ngrams(s):\n",
    "#     tokens = nltk.word_tokenize(s)\n",
    "    \n",
    "\n",
    "# df['question_ngram'] = dfp['question_stem'].apply(\n",
    "#     lambda x: process_ngrams(x)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
